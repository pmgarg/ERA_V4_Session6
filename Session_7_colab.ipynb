{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM9xGlkVS20HqjqAPQiO4tY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pmgarg/ERA_V4_Session6/blob/main/Session_7_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "8ixGfbjiVy3r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "import random\n",
        "from PIL import Image\n",
        "import platform\n",
        "import argparse\n",
        "import sys\n",
        "import os\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET = 'CIFAR-10'\n",
        "NUM_CLASSES = 10\n",
        "IMAGE_SIZE = 32\n",
        "MEAN = (0.4914, 0.4822, 0.4465)\n",
        "STD = (0.2470, 0.2435, 0.2616)\n",
        "\n",
        "INITIAL_CHANNELS = 3\n",
        "\n",
        "# Training\n",
        "BATCH_SIZE = 128\n",
        "LEARNING_RATE = 0.01\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 1e-4\n",
        "NUM_EPOCHS = 50\n",
        "\n",
        "# Augmentation\n",
        "USE_AUGMENTATION = True"
      ],
      "metadata": {
        "id": "NQR1PfxLXwEN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.backends.mps.is_available():\n",
        "    DEVICE = 'mps'\n",
        "elif torch.cuda.is_available():\n",
        "    DEVICE = 'cuda'\n",
        "else:\n",
        "    DEVICE = 'cpu'"
      ],
      "metadata": {
        "id": "hq3KPKS8WwvI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CutoutTransform:\n",
        "    \"\"\"Custom implementation of Cutout/CoarseDropout using torchvision\"\"\"\n",
        "    def __init__(self, n_holes=1, length=16, fill_value=None):\n",
        "        self.n_holes = n_holes\n",
        "        self.length = length\n",
        "        self.fill_value = fill_value\n",
        "\n",
        "    def __call__(self, img):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            img (PIL Image or Tensor): Image to apply cutout\n",
        "        Returns:\n",
        "            PIL Image or Tensor: Image with cutout applied\n",
        "        \"\"\"\n",
        "        if isinstance(img, Image.Image):\n",
        "            img = np.array(img)\n",
        "            was_pil = True\n",
        "        else:\n",
        "            was_pil = False\n",
        "\n",
        "        h, w = img.shape[:2]\n",
        "\n",
        "        # Use dataset mean if no fill value provided\n",
        "        if self.fill_value is None:\n",
        "            self.fill_value = [125, 122, 113]  # CIFAR-10 approximate means in 0-255 range\n",
        "\n",
        "        for _ in range(self.n_holes):\n",
        "            y = np.random.randint(h)\n",
        "            x = np.random.randint(w)\n",
        "\n",
        "            y1 = max(0, y - self.length // 2)\n",
        "            y2 = min(h, y + self.length // 2)\n",
        "            x1 = max(0, x - self.length // 2)\n",
        "            x2 = min(w, x + self.length // 2)\n",
        "\n",
        "            # Apply cutout\n",
        "            img[y1:y2, x1:x2] = self.fill_value\n",
        "\n",
        "        if was_pil:\n",
        "            return Image.fromarray(img)\n",
        "        return img\n",
        "\n",
        "\n",
        "class ShiftScaleRotate:\n",
        "    \"\"\"Custom implementation of ShiftScaleRotate using torchvision\"\"\"\n",
        "    def __init__(self, shift_limit=0.1, scale_limit=0.1, rotate_limit=15, p=0.5):\n",
        "        self.shift_limit = shift_limit\n",
        "        self.scale_limit = scale_limit\n",
        "        self.rotate_limit = rotate_limit\n",
        "        self.p = p\n",
        "\n",
        "    def __call__(self, img):\n",
        "        if random.random() > self.p:\n",
        "            return img\n",
        "\n",
        "        # Random parameters\n",
        "        angle = random.uniform(-self.rotate_limit, self.rotate_limit)\n",
        "        scale = random.uniform(1 - self.scale_limit, 1 + self.scale_limit)\n",
        "\n",
        "        # Get image dimensions\n",
        "        width, height = img.size if isinstance(img, Image.Image) else (img.shape[1], img.shape[0])\n",
        "\n",
        "        # Calculate shift\n",
        "        max_dx = self.shift_limit * width\n",
        "        max_dy = self.shift_limit * height\n",
        "        dx = random.uniform(-max_dx, max_dx)\n",
        "        dy = random.uniform(-max_dy, max_dy)\n",
        "\n",
        "        # Apply transformations using torchvision\n",
        "        if isinstance(img, Image.Image):\n",
        "            # Apply affine transformation\n",
        "            img = transforms.functional.affine(\n",
        "                img,\n",
        "                angle=angle,\n",
        "                translate=(dx, dy),\n",
        "                scale=scale,\n",
        "                shear=0\n",
        "            )\n",
        "\n",
        "        return img\n"
      ],
      "metadata": {
        "id": "wz_lSHQDYOkN"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fill_value = tuple([int(m * 255) for m in MEAN])\n",
        "train_transform = transforms.Compose([\n",
        "        transforms.RandomHorizontalFlip(p=0.5),\n",
        "        ShiftScaleRotate(\n",
        "            shift_limit=0.1,\n",
        "            scale_limit=0.1,\n",
        "            rotate_limit=15,\n",
        "            p=0.5\n",
        "        ),\n",
        "        CutoutTransform(\n",
        "            n_holes=1,\n",
        "            length=16,\n",
        "            fill_value=fill_value\n",
        "        ),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD)\n",
        "    ])\n",
        "val_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=MEAN, std=STD)\n",
        "    ])"
      ],
      "metadata": {
        "id": "ljlIqpreXgE9"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=True,\n",
        "        transform=train_transform,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "val_dataset = datasets.CIFAR10(\n",
        "        root='./data',\n",
        "        train=False,\n",
        "        transform=val_transform,\n",
        "        download=True\n",
        "    )\n",
        "\n",
        "train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=128,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=128,\n",
        "        shuffle=False\n",
        "    )"
      ],
      "metadata": {
        "id": "U1cruVtZYwRH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d5f645a-65ee-40e2-d442-7a80ca5bfb43"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [01:15<00:00, 2.26MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class CIFAR10_CNN(nn.Module):\n",
        "    \"\"\"\n",
        "    Advanced CNN for CIFAR-10 with:\n",
        "    - C1C2C3C4 architecture (no MaxPooling)\n",
        "    - Dilated convolutions for downsampling\n",
        "    - Depthwise Separable Convolution\n",
        "    - Global Average Pooling\n",
        "    - Total RF > 44\n",
        "    - Parameters < 200k\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_classes=10):\n",
        "        super().__init__()\n",
        "\n",
        "        # C1: Initial convolution block (RF: 3)\n",
        "        self.c1 = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=3,stride=1, padding=1,bias=False), # RF: 3\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.Conv2d(32, 32, kernel_size=3,stride=1, padding=1,bias=False), # RF: 5\n",
        "            nn.BatchNorm2d(32),\n",
        "        )\n",
        "\n",
        "        # C2: Depthwise Separable Convolution block (RF increases)\n",
        "        self.c2 = nn.Sequential(\n",
        "                  nn.Conv2d(32, 32, kernel_size=3, stride=1, padding=1, groups=32,\n",
        "                            bias=False),\n",
        "                  #  convolution (1x1)\n",
        "                  nn.Conv2d(32, 64, kernel_size=1, bias=False),# RF: 7\n",
        "\n",
        "                  nn.BatchNorm2d(64),\n",
        "                  nn.Conv2d(64, 64, kernel_size=3,stride=1, padding=1,bias=False), # RF: 9\n",
        "                  nn.BatchNorm2d(64),\n",
        "                )\n",
        "\n",
        "        # C3: Standard convolution with dilated conv (RF increases significantly)\n",
        "        self.c3 = nn.Sequential(\n",
        "            nn.Conv2d(64, 128, kernel_size=3,stride=1, padding=1,bias=False), # RF: 11\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, kernel_size=3, padding=2, dilation=2, bias=False), # RF: 15 (dilated)\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.Conv2d(128, 128, kernel_size=3,stride=1, padding=1,bias=False), # RF: 5\n",
        "            nn.BatchNorm2d(128),\n",
        "        )\n",
        "\n",
        "        # C4: Final block with dilated convolution for downsampling (instead of stride)\n",
        "        # Using dilated convolutions with higher dilation for effective downsampling\n",
        "        self.c4 = nn.Sequential(\n",
        "            nn.Conv2d(128, 256, kernel_size=3, padding=4, dilation=4, bias=False), # RF: 25 (high dilation)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, kernel_size=3,stride=1, padding=1,bias=False), # RF: 27\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, kernel_size=3, padding=8, dilation=8, bias=False), # RF: 43 (very high dilation)\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.Conv2d(256, 256, kernel_size=1,stride=1, padding=0,bias=False), # 1x1 conv, RF unchanged\n",
        "            nn.BatchNorm2d(256),\n",
        "        )\n",
        "\n",
        "        # Global Average Pooling\n",
        "        self.gap = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        # Fully Connected layer after GAP\n",
        "        self.fc = nn.Linear(256, num_classes)\n",
        "\n",
        "        # Initialize weights\n",
        "        #self._initialize_weights()\n",
        "\n",
        "    def _initialize_weights(self):\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.constant_(m.weight, 1)\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.c1(x))\n",
        "        x = F.relu(self.c2(x))\n",
        "        x = F.relu(self.c3(x))\n",
        "        x = F.relu(self.c4(x))\n",
        "        x = self.gap(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "    def get_receptive_field(self):\n",
        "        \"\"\"Calculate and return the total receptive field\"\"\"\n",
        "        # With the dilated convolutions:\n",
        "        # C1: RF = 5\n",
        "        # C2: RF = 9\n",
        "        # C3: RF = 17 (with dilation=2)\n",
        "        # C4: RF = 43+ (with dilation=4 and 8)\n",
        "        # Total RF > 44 ✓\n",
        "        return 45\n"
      ],
      "metadata": {
        "id": "xS-QeCJTaZeh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Trainer:\n",
        "    \"\"\"Trainer class for the CNN model\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model.to(DEVICE)\n",
        "        self.device = DEVICE\n",
        "\n",
        "        self.criterion = nn.CrossEntropyLoss()\n",
        "        self.optimizer = optim.SGD(\n",
        "            self.model.parameters(),\n",
        "            lr=LEARNING_RATE,\n",
        "            momentum=MOMENTUM,\n",
        "            weight_decay=WEIGHT_DECAY\n",
        "        )\n",
        "\n",
        "        self.best_accuracy = 0\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        \"\"\"Train for one epoch\"\"\"\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc='Training')\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': running_loss / len(pbar),\n",
        "                'acc': 100. * correct / total\n",
        "            })\n",
        "\n",
        "        return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "    def train_epoch_with_scheduler(self, train_loader):\n",
        "        \"\"\"Train for one epoch with scheduler step\"\"\"\n",
        "        self.model.train()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc='Training')\n",
        "        for inputs, labels in pbar:\n",
        "            inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "            self.optimizer.zero_grad()\n",
        "            outputs = self.model(inputs)\n",
        "            loss = self.criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "            # Step scheduler after each batch\n",
        "            if hasattr(self, 'scheduler'):\n",
        "                self.scheduler.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            _, predicted = outputs.max(1)\n",
        "            total += labels.size(0)\n",
        "            correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "            pbar.set_postfix({\n",
        "                'loss': running_loss / len(pbar),\n",
        "                'acc': 100. * correct / total\n",
        "            })\n",
        "\n",
        "        return running_loss / len(train_loader), 100. * correct / total\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        \"\"\"Validate the model\"\"\"\n",
        "        self.model.eval()\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pbar = tqdm(val_loader, desc='Validation')\n",
        "            for inputs, labels in pbar:\n",
        "                inputs, labels = inputs.to(self.device), labels.to(self.device)\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self.criterion(outputs, labels)\n",
        "\n",
        "                running_loss += loss.item()\n",
        "                _, predicted = outputs.max(1)\n",
        "                total += labels.size(0)\n",
        "                correct += predicted.eq(labels).sum().item()\n",
        "\n",
        "                pbar.set_postfix({\n",
        "                    'loss': running_loss / len(pbar),\n",
        "                    'acc': 100. * correct / total\n",
        "                })\n",
        "\n",
        "        accuracy = 100. * correct / total\n",
        "        return running_loss / len(val_loader), accuracy\n",
        "\n",
        "    def train(self, train_loader, val_loader, num_epochs):\n",
        "        \"\"\"Full training loop\"\"\"\n",
        "\n",
        "        # Setup learning rate scheduler\n",
        "        total_steps = num_epochs * len(train_loader)\n",
        "        self.scheduler = OneCycleLR(\n",
        "            self.optimizer,\n",
        "            max_lr=0.1,\n",
        "            total_steps=total_steps,\n",
        "            pct_start=0.3,\n",
        "            anneal_strategy='cos'\n",
        "        )\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            print(f'\\nEpoch {epoch+1}/{num_epochs}')\n",
        "\n",
        "            # Train\n",
        "            train_loss, train_acc = self.train_epoch_with_scheduler(train_loader)\n",
        "\n",
        "            # Validate\n",
        "            val_loss, val_acc = self.validate(val_loader)\n",
        "\n",
        "            print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%')\n",
        "            print(f'Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%')\n",
        "\n",
        "            # Save best model\n",
        "            if val_acc > self.best_accuracy:\n",
        "                self.best_accuracy = val_acc\n",
        "                self.save_checkpoint(epoch, val_acc)\n",
        "                print(f'Best model saved! Accuracy: {val_acc:.2f}%')\n",
        "\n",
        "    def save_checkpoint(self, epoch, accuracy):\n",
        "        \"\"\"Save model checkpoint\"\"\"\n",
        "        os.makedirs('./checkpoints', exist_ok=True)\n",
        "\n",
        "        checkpoint = {\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': self.model.state_dict(),\n",
        "            'optimizer_state_dict': self.optimizer.state_dict(),\n",
        "            'accuracy': accuracy,\n",
        "        }\n",
        "\n",
        "        path = os.path.join('./checkpoints', f'best_model.pth')\n",
        "        torch.save(checkpoint, path)\n"
      ],
      "metadata": {
        "id": "oT-CjwLQ1e37"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count_parameters(model):\n",
        "    \"\"\"Count the number of trainable parameters\"\"\"\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "\n",
        "def calculate_receptive_field(model):\n",
        "    \"\"\"Calculate theoretical receptive field\"\"\"\n",
        "    rf = 1\n",
        "    stride = 1\n",
        "\n",
        "    # This is a simplified calculation\n",
        "    # For accurate RF, trace through each layer\n",
        "    layers_info = [\n",
        "        (3, 1, 1),  # kernel, stride, padding\n",
        "        (3, 1, 1),\n",
        "        (3, 1, 1),\n",
        "        (3, 1, 1),\n",
        "        (3, 1, 1),\n",
        "        (3, 2, 1),  # dilated conv acts like larger kernel\n",
        "        (3, 1, 1),\n",
        "        (3, 4, 1),  # dilated conv\n",
        "        (3, 1, 1),\n",
        "        (3, 8, 1),  # dilated conv\n",
        "    ]\n",
        "\n",
        "    for k, s, _ in layers_info:\n",
        "        rf = rf + (k - 1) * stride\n",
        "        stride = stride * s\n",
        "\n",
        "    return rf\n",
        "\n",
        "\n",
        "def print_model_summary(model):\n",
        "    \"\"\"Print model summary\"\"\"\n",
        "    num_params = count_parameters(model)\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Model: CIFAR-10 Advanced CNN\")\n",
        "    print(f\"{'='*50}\")\n",
        "    print(f\"Total Parameters: {num_params:,}\")\n",
        "    print(f\"Receptive Field: {model.get_receptive_field()}\")\n",
        "    print(f\"Architecture: C1-C2-C3-C4-GAP-FC\")\n",
        "    print(f\"Depthwise Separable Conv: ✓ (in C2)\")\n",
        "    print(f\"Dilated Convolution: ✓ (in C3 and C4)\")\n",
        "    print(f\"Global Average Pooling: ✓\")\n",
        "    print(f\"Target Accuracy: 85%\")\n",
        "    print(f\"Parameter Limit: 200,000\")\n",
        "    print(f\"Parameter Check: {'✓ PASS' if num_params < 200000 else '✗ FAIL'}\")\n",
        "    print(f\"{'='*50}\\n\")"
      ],
      "metadata": {
        "id": "PoTDOKhS1sTL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "    if torch.backends.mps.is_available():\n",
        "        DEVICE = 'mps'\n",
        "        print(\"Using MPS (Metal Performance Shaders) device\")\n",
        "    elif torch.cuda.is_available():\n",
        "        DEVICE = 'cuda'\n",
        "        print(\"Using CUDA device\")\n",
        "    else:\n",
        "        DEVICE = 'cpu'\n",
        "        print(\"Using CPU device\")\n",
        "\n",
        "    # Model\n",
        "    model = CIFAR10_CNN(num_classes=10)\n",
        "    print_model_summary(model)\n",
        "\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(model)\n",
        "\n",
        "    # Train\n",
        "    trainer.train(train_loader, val_loader, NUM_EPOCHS)\n",
        "\n",
        "    print(f\"\\n{'='*50}\")\n",
        "    print(f\"Training Complete!\")\n",
        "    print(f\"Best Validation Accuracy: {trainer.best_accuracy:.2f}%\")\n",
        "    print(f\"{'='*50}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fg5J6Cb6156u",
        "outputId": "57c18b05-8c21-4356-91a8-5b9023cd16c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using CUDA device\n",
            "\n",
            "==================================================\n",
            "Model: CIFAR-10 Advanced CNN\n",
            "==================================================\n",
            "Total Parameters: 1,963,786\n",
            "Receptive Field: 45\n",
            "Architecture: C1-C2-C3-C4-GAP-FC\n",
            "Depthwise Separable Conv: ✓ (in C2)\n",
            "Dilated Convolution: ✓ (in C3 and C4)\n",
            "Global Average Pooling: ✓\n",
            "Target Accuracy: 85%\n",
            "Parameter Limit: 200,000\n",
            "Parameter Check: ✗ FAIL\n",
            "==================================================\n",
            "\n",
            "\n",
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:34<00:00,  2.53it/s, loss=1.6, acc=40.4]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.63it/s, loss=1.35, acc=51.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.6040, Train Acc: 40.45%\n",
            "Val Loss: 1.3487, Val Acc: 51.12%\n",
            "Best model saved! Accuracy: 51.12%\n",
            "\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:38<00:00,  2.47it/s, loss=1.31, acc=52.4]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.50it/s, loss=1.12, acc=60.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.3118, Train Acc: 52.41%\n",
            "Val Loss: 1.1219, Val Acc: 60.06%\n",
            "Best model saved! Accuracy: 60.06%\n",
            "\n",
            "Epoch 3/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=1.2, acc=57]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.47it/s, loss=1.21, acc=57.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1985, Train Acc: 56.96%\n",
            "Val Loss: 1.2116, Val Acc: 57.33%\n",
            "\n",
            "Epoch 4/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=1.14, acc=59.1]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.52it/s, loss=1.03, acc=63.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.1368, Train Acc: 59.13%\n",
            "Val Loss: 1.0337, Val Acc: 63.63%\n",
            "Best model saved! Accuracy: 63.63%\n",
            "\n",
            "Epoch 5/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=1.09, acc=61.2]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.54it/s, loss=1.24, acc=59.9]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0893, Train Acc: 61.23%\n",
            "Val Loss: 1.2430, Val Acc: 59.85%\n",
            "\n",
            "Epoch 6/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=1.04, acc=63.1]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.52it/s, loss=0.944, acc=67]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 1.0401, Train Acc: 63.07%\n",
            "Val Loss: 0.9439, Val Acc: 66.96%\n",
            "Best model saved! Accuracy: 66.96%\n",
            "\n",
            "Epoch 7/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=0.985, acc=65.2]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.53it/s, loss=0.829, acc=70.6]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9846, Train Acc: 65.17%\n",
            "Val Loss: 0.8286, Val Acc: 70.58%\n",
            "Best model saved! Accuracy: 70.58%\n",
            "\n",
            "Epoch 8/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=0.938, acc=66.7]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.52it/s, loss=0.817, acc=72.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.9383, Train Acc: 66.74%\n",
            "Val Loss: 0.8170, Val Acc: 72.08%\n",
            "Best model saved! Accuracy: 72.08%\n",
            "\n",
            "Epoch 9/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=0.897, acc=68.3]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.51it/s, loss=0.808, acc=72.5]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8967, Train Acc: 68.31%\n",
            "Val Loss: 0.8076, Val Acc: 72.53%\n",
            "Best model saved! Accuracy: 72.53%\n",
            "\n",
            "Epoch 10/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=0.863, acc=69.7]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.56it/s, loss=0.684, acc=76.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8634, Train Acc: 69.72%\n",
            "Val Loss: 0.6835, Val Acc: 76.35%\n",
            "Best model saved! Accuracy: 76.35%\n",
            "\n",
            "Epoch 11/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=0.839, acc=70.7]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.57it/s, loss=0.689, acc=76.3]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.8388, Train Acc: 70.72%\n",
            "Val Loss: 0.6889, Val Acc: 76.29%\n",
            "\n",
            "Epoch 12/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=0.799, acc=71.8]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.58it/s, loss=0.709, acc=76]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7990, Train Acc: 71.79%\n",
            "Val Loss: 0.7088, Val Acc: 76.05%\n",
            "\n",
            "Epoch 13/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.45it/s, loss=0.784, acc=72.5]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.55it/s, loss=0.638, acc=77.1]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7844, Train Acc: 72.50%\n",
            "Val Loss: 0.6376, Val Acc: 77.10%\n",
            "Best model saved! Accuracy: 77.10%\n",
            "\n",
            "Epoch 14/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.46it/s, loss=0.763, acc=73.1]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.56it/s, loss=0.692, acc=76.4]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7632, Train Acc: 73.13%\n",
            "Val Loss: 0.6925, Val Acc: 76.44%\n",
            "\n",
            "Epoch 15/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|██████████| 391/391 [02:39<00:00,  2.46it/s, loss=0.746, acc=73.9]\n",
            "Validation: 100%|██████████| 79/79 [00:10<00:00,  7.57it/s, loss=0.596, acc=80]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7463, Train Acc: 73.90%\n",
            "Val Loss: 0.5957, Val Acc: 80.03%\n",
            "Best model saved! Accuracy: 80.03%\n",
            "\n",
            "Epoch 16/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training:  59%|█████▉    | 232/391 [01:34<01:04,  2.47it/s, loss=0.434, acc=74.5]"
          ]
        }
      ]
    }
  ]
}